# 1. Import Libraries & Define Functions

from IPython.core.display import display, HTML
display(HTML("<style>.container { width:90% !important; }</style>"))

%pylab inline

import dataiku
from dataiku import pandasutils as pdu
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy.stats import norm
from scipy import stats
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

def corr_feature_selection(df, corr_method = 'spearman', thresh = 0.9):
    """
        Stepwise varaible selection using bivariate relationship

        Parameters:
        df (dataframe): dataframe of independent variables
        corr_method (str): 'spearman', 'pearson', 'kendall'
        thresh (demical): count corrleated varaibles with correlation >= thresh

        Returns:
        df (dataframe): Return dataframe with non-correlated varaibles
    """
    corr_num = 2
    while corr_num > 1:
        
        corrmat = df.corr(method = corr_method)
        corr_df = corrmat[abs(corrmat) >= thresh].count().sort_values(ascending = False).reset_index()
        corr_df.columns = ['variable', 'corr_var_num']
        corr_num = corr_df['corr_var_num'][0]
        
        if corr_num > 1:
            drop_col = corr_df['variable'][0]
            df = df.drop(columns=[drop_col])
        else:
            return df

def vif_feature_selection(df, thresh = 5):
    """
        Stepwise varaible selection using VIF

        Parameters:
        df (dataframe): dataframe of independent variables
        thresh (float): remove varaibles with VIF > = thresh

        Returns:
        df (dataframe): Return dataframe with non-correlated varaibles
    """    
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    from statsmodels.tools.tools import add_constant

    vif_value = 100
    while vif_value > thresh:
        
        df_num = df._get_numeric_data()
        numeric_col = df_num.columns
        X = add_constant(df_num)
        df_vif = pd.DataFrame([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], 
                  index=X.columns).reset_index()
        vif_value = df_vif.iloc[1,1]
        
        if vif_value > thresh:
            drop_col = df_vif.iloc[1,0]
            df = df.drop(columns=[drop_col])
        else:
            return df

# 2. Prepare Data

# Read the dataset as a Pandas dataframe in memory
# Note: here, we only read the first 100K rows. Other sampling options are available
df = dataiku.Dataset("2019_Novantas_copy").get_dataframe()

df_drop = df.drop(['Row Labels', 'micro_mkt'], axis=1)

#missing data
total = df_drop.isnull().sum().sort_values(ascending=False)
percent = (100*df_drop.isnull().sum()/df_drop.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Missing Percent'])
missing_data.head()

# 3. Feature Selection: PCA

df_corr = corr_feature_selection(df_drop)

pca_percent = 0.9
pipeline = Pipeline([('scaling', MinMaxScaler()), ('pca', PCA(pca_percent))])
df_pca = pd.DataFrame(pipeline.fit_transform(df_corr))
df_pca.shape

# 4. Clustering Algorihtms

## 4.1 K-means

from sklearn.cluster import KMeans
wss = []
k_range = 10

for k in range(1, k_range):
    kmeans = KMeans(n_clusters=k, random_state=1).fit(df_pca)
    print(kmeans.inertia_)
    wss.append(np.sqrt(kmeans.inertia_))
    
plt.figure(figsize=(10,10))
plt.plot(range(1, k_range), wss, marker='s')
plt.xlabel('$k$')
plt.ylabel('$J(C_k)$');

- Optimal number of clusters is 3

from sklearn.cluster import KMeans
sil = []
k_range = 10

for k in range(2, k_range):
    kmeans = KMeans(n_clusters=k, random_state=1).fit(df_pca)
    sil.append(silhouette_score(df_pca, kmeans.labels_))
    
plt.figure(figsize=(10,10))
plt.plot(range(2, k_range), sil, marker='s')
plt.xlabel('# of Clusters')
plt.ylabel('Silhouette');

## 4.2 AffinityPropagation

# affinity propagation clustering
from sklearn.cluster import AffinityPropagation

sil = []
damp = np.arange(0.5, 0.9, 0.05)

for i in damp:
    aff_prop = AffinityPropagation(damping = i).fit(df_pca)
    sil.append(metrics.silhouette_score(df_pca, aff_prop.labels_))
    
plt.figure(figsize=(10,10))
plt.plot(damp, sil, marker='s')
plt.xlabel('Damp')
plt.ylabel('Silhouette');

## 4.3 Agglomerative

from sklearn.cluster import KMeans
sil = []
k_range = 10

for k in range(2, k_range):
    agg_clust = AgglomerativeClustering(n_clusters=k).fit(df_pca)
    sil.append(metrics.silhouette_score(df_pca, agg_clust.labels_))
    
plt.figure(figsize=(10,10))
plt.plot(range(2, k_range), sil, marker='s')
plt.xlabel('# of Clusters')
plt.ylabel('Silhouette');

## 4.4 BIRCH

from sklearn.cluster import Birch

sil_dict = {}
sil_list = []
k_range = 10
thresh = np.arange(0.5, 1, 0.05)


for i in thresh:
    for k in range(2, k_range):
        birch = Birch(threshold = i, n_clusters=k).fit(df_pca)
        sil_dict[k] = metrics.silhouette_score(df_pca, birch.labels_)
    sil_list.append(sil_dict)
    inertia = {}
    
sil_df = pd.DataFrame(sil_list, index = thresh)
sil_df.describe()

# plt.figure(figsize=(10,10))
# plt.plot(range(2, k_range), inertia, marker='s')
# plt.xlabel('# of Clusters')
# plt.ylabel('Silhouette');

- It seems that this algo doesn’t offer many advantages over K-means, unless your dataset is really big and doesn’t fit in memory.

## 4.5 DBSCAN

from sklearn.cluster import DBSCAN
from itertools import product

eps_values = np.arange(.3, 1, 0.05) # eps values to be investigated
min_samples = np.arange(3, 10) # min_samples values to be investigated
DBSCAN_params = list(product(eps_values, min_samples))

no_of_clusters = []
sil_score = []

for p in DBSCAN_params:
    DBS_clustering = DBSCAN(eps=p[0], min_samples=p[1]).fit(df_pca)
    no_of_clusters.append(len(np.unique(DBS_clustering.labels_)))
    sil_score.append(metrics.silhouette_score(df_pca, DBS_clustering.labels_))

tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   
tmp['No_of_clusters'] = no_of_clusters

pivot_1 = pd.pivot_table(tmp, values='No_of_clusters', index='Min_samples', columns='Eps')

fig, ax = plt.subplots(figsize=(12,6))
sns.heatmap(pivot_1, annot=True,annot_kws={"size": 16}, cmap="YlGnBu", ax=ax)
ax.set_title('Number of clusters')
plt.show()

tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   
tmp['Sil_score'] = sil_score

pivot_1 = pd.pivot_table(tmp, values='Sil_score', index='Min_samples', columns='Eps')

fig, ax = plt.subplots(figsize=(18,6))
sns.heatmap(pivot_1, annot=True, annot_kws={"size": 10}, cmap="YlGnBu", ax=ax)
plt.show()

## 4.6 Gauss-Mixture

from sklearn.mixture import GaussianMixture

sil = []
k_range = 10

for k in range(2, k_range):
    gauss = GaussianMixture(n_components=k).fit(df_pca)
    sil.append(metrics.silhouette_score(df_pca, gauss.predict(df_pca)))
    
plt.figure(figsize=(10,10))
plt.plot(range(2, k_range), sil, marker='s')
plt.xlabel('# of Clusters')
plt.ylabel('Silhouette');

## 4.7 Summary

from sklearn import metrics
from sklearn import datasets
import pandas as pd
from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation, SpectralClustering


data = datasets.load_digits()
X, y = data.data, data.target

algorithms = []
algorithms.append(KMeans(n_clusters=4, random_state=1))
algorithms.append(AgglomerativeClustering(n_clusters=4))

data = []
for algo in algorithms:
    algo.fit(X)
    data.append(({
        'ARI': metrics.adjusted_rand_score(y, algo.labels_),
        'AMI': metrics.adjusted_mutual_info_score(y, algo.labels_,
                                                 average_method='arithmetic'),
        'Homogenity': metrics.homogeneity_score(y, algo.labels_),
        'Completeness': metrics.completeness_score(y, algo.labels_),
        'V-measure': metrics.v_measure_score(y, algo.labels_),
        'Silhouette': metrics.silhouette_score(X, algo.labels_)}))

results = pd.DataFrame(data=data, columns=['ARI', 'AMI', 'Homogenity',
                                           'Completeness', 'V-measure', 
                                           'Silhouette'],
                       index=['K-means', 'Agglomerative'])

results
