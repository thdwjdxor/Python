# Treat NA -----------------------------------------------------------#
housing.dropna(subset=["total_bedrooms"]) # Remove row with NA
housing.drop("total_bedrooms", axis=1) # Remove entire Column

housing["total_bedrooms"].fillna(median) # fill-in with median

# OR sklearn fill-in with median
from sklearn.preprocessing import Imputer
imputer = Imputer(strategy="median")
housing_num = housing.drop("ocean_proximity", axis=1) # Remove Categorical columns
imputer.fit(housing_num) # fill-in with median for all columns

imputer.statistics_ # contains medians for each column. Save this so you can apply to test dataset too
housing_num.median().values # same value as above

X = imputer.transform(housing_num) # Replace NA with medians
housing_tr = pd.DataFrame(X, columns=housing_num.columns) # convert numpy array to Pandas Dataframe


# Cateogory to Numeric  -----------------------------------------------------------#
# Option 1: Text to number
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded = encoder.fit_transform(housing_cat)
housing_cat_encoded
print(encoder.classes_) # Mapping

# Option 2: One-Hot-Encoder. Continues from Option 1
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()
housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))
housing_cat_1hot.toarray()

# Combine Option 1 and 2:
from sklearn.preprocessing import LabelBinarizer
housing_cat = housing["ocean_proximity"]
encoder = LabelBinarizer()
housing_cat_1hot = encoder.fit_transform(housing_cat)
housing_cat_1hot


# Pipeline to Combine transfomrs ----------------------------------------------#
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

from sklearn.pipeline import FeatureUnion
num_attribs = list(housing_num)
cat_attribs = ["ocean_proximity"]
num_pipeline = Pipeline([
    ('selector', DataFrameSelector(num_attribs)),
    ('imputer', Imputer(strategy="median")),
    ('attribs_adder', CombinedAttributesAdder()),
    ('std_scaler', StandardScaler()),
  ])
cat_pipeline = Pipeline([
    ('selector', DataFrameSelector(cat_attribs)),
    ('label_binarizer', LabelBinarizer()),
  ])
full_pipeline = FeatureUnion(transformer_list=[
    ("num_pipeline", num_pipeline),
    ("cat_pipeline", cat_pipeline),
  ])

housing_prepared = full_pipeline.fit_transform(housing)


# Split Train and Test Dataset -----------------------------------#
# Option 1:
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]

import numpy as np
shuffle_index = np.random.permutation(60000)
X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]

# Option 2:
np.random.seed(42)

import numpy as np
def split_train_test(data, test_ratio):
  shuffled_indices = np.random.permutation(len(data))
  test_set_size = int(len(data) * test_ratio)
  test_indices = shuffled_indices[:test_set_size]
  train_indices = shuffled_indices[test_set_size:]
  return data.iloc[train_indices], data.iloc[test_indices]
  
train_set, test_set = split_train_test(housing, 0.2)


# Corelation Feature Selection  -----------------------------------#
def corr_feature_selection(df, corr_method = 'spearman', thresh = 0.9):
    """
        Stepwise varaible selection using bivariate relationship

        Parameters:
        df (dataframe): dataframe of independent variables
        corr_method (str): 'spearman', 'pearson', 'kendall'
        thresh (demical): count corrleated varaibles with correlation >= thresh

        Returns:
        df (dataframe): Return dataframe with non-correlated varaibles
    """
    corr_num = 2
    while corr_num > 1:
        
        corrmat = df.corr(method = corr_method)
        corr_df = corrmat[abs(corrmat) >= thresh].count().sort_values(ascending = False).reset_index()
        corr_df.columns = ['variable', 'corr_var_num']
        corr_num = corr_df['corr_var_num'][0]
        
        if corr_num > 1:
            drop_col = corr_df['variable'][0]
            df = df.drop(columns=[drop_col])
        else:
            return df
            

# VIF Feature Selection  -----------------------------------#
def vif_feature_selection(df, thresh = 5):
    """
        Stepwise varaible selection using VIF

        Parameters:
        df (dataframe): dataframe of independent variables
        thresh (float): remove varaibles with VIF > = thresh

        Returns:
        df (dataframe): Return dataframe with non-correlated varaibles
    """    
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    from statsmodels.tools.tools import add_constant

    vif_value = 100
    while vif_value > thresh:
        
        df_num = df._get_numeric_data()
        numeric_col = df_num.columns
        X = add_constant(df_num)
        df_vif = pd.DataFrame([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], 
                  index=X.columns).reset_index()
        vif_value = df_vif.iloc[1,1]
        
        if vif_value > thresh:
            drop_col = df_vif.iloc[1,0]
            df = df.drop(columns=[drop_col])
        else:
            return df
            

# XGB Feature Selection  -----------------------------------#
1) Permutation_Importance on full-dataset to idenfity variables: simplest

from sklearn.inspection import permutation_importance
result = permutation_importance(XGBRegressor().fit(df,y), df, y, random_state = 0, scoring='neg_mean_squared_error')
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=df.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
plt.show()

2) Permutation_Importance cv to idenfity variables: simplest
import eli5
from eli5.sklearn import PermutationImportance
    
svc = XGBRegressor(random_state = 0)
perm = PermutationImportance(svc, cv = 5).fit(np.matrix(df), np.matrix(y))
eli5.show_weights(perm)

3) Permutation_Importance cv to calculate model accuracy(RMSE) per n_features: complex
def sup_feature_selection(model_choice, df, y, thresh = 5, cv = 5, shuf = False, rand_state = None):
    """
        Calculate rmse of each cv 

        Parameters:
        model (model): e.i. RandomForestRegressor(), XGBRegressor()
        df (dataframe): dataframe of independent variables
        y (dataframe): dataframe of dependent variables
        cv (int): # of CV to calculate average RMSE, and average Permuatation Importance per varaible
        shuf (boolean) = shuffle observation
        rand_state (int) = any integer

        Returns:
        rmse_list (list): list of rmse values from cv
    """
    # Import KFold
    from sklearn.inspection import permutation_importance
    from sklearn.model_selection import KFold
    from sklearn.datasets import make_classification
    from sklearn.metrics import mean_squared_error
    import random

    
    rmse_list1 = list()
    n_features = list(reversed(range(thresh, len(df.columns) + 1)))
    
    if shuf == False:
        
        imp_num = len(df.columns)
        while imp_num >= thresh:
            
            X = np.matrix(df)
            y = np.matrix(y)
            perm = np.array(range(df.shape[1]))
            rmse_list = list()
            print(df.columns)

            cv_outer = KFold(n_splits = cv)

            for train_ix, test_ix in cv_outer.split(X):
                X_train, X_test = X[train_ix, :], X[test_ix, :]
                y_train, y_test = y[train_ix], y[test_ix]

                model = model_choice.fit(X_train, y_train)
                prediction = model.predict(X_test)
                rmse_list.append(np.sqrt(mean_squared_error(y_test, prediction)))
                
            model = model_choice.fit(df,y)                
            r = permutation_importance(model, df, y)            
            min_imp = r.importances_mean.argmin()
            drop_col = df.columns[min_imp]
            df = df.drop(columns=[drop_col])
            
            rmse_list1.append(np.mean(rmse_list))
            imp_num = len(df.columns)
            perm = np.array(range(df.shape[1]))
            rmse_list = list()

        df_summary = pd.DataFrame(rmse_list1, n_features).reset_index()
        df_summary.columns = ['n_features', 'RMSE']
        return df_summary
        
    if shuf == True:            

        imp_num = len(df.columns)
        while imp_num >= thresh:
            
            X = np.matrix(df)
            y = np.matrix(y)
            perm = np.array(range(df.shape[1]))
            rmse_list = list()
            print(df.columns)

            cv_outer = KFold(n_splits = cv, shuffle = True, random_state = rand_state)  

            for train_ix, test_ix in cv_outer.split(X):
                X_train, X_test = X[train_ix, :], X[test_ix, :]
                y_train, y_test = y[train_ix], y[test_ix]

                model = model_choice.fit(X_train, y_train)
                prediction = model.predict(X_test)
                rmse_list.append(np.sqrt(mean_squared_error(y_test, prediction)))
                
            model = model_choice.fit(df,y)                
            r = permutation_importance(model, df, y)            
            min_imp = r.importances_mean.argmin()
            drop_col = df.columns[min_imp]
            df = df.drop(columns=[drop_col])
            
            rmse_list1.append(np.mean(rmse_list))
            imp_num = len(df.columns)
            perm = np.array(range(df.shape[1]))
            rmse_list = list()

        df_summary = pd.DataFrame(rmse_list1, n_features).reset_index()
        df_summary.columns = ['n_features', 'RMSE']
        return df_summary
    
from xgboost import XGBRegressor
sup_feature_selection(XGBRegressor(), df,y, shuf = False, cv = 5)


# time based split -----------------------------------#

np.random.seed(42)

# Sort Data
df1 = df1.sort_values('TREND').reset_index().drop('index',axis=1)
df1.head()

# Find row where to split test dataset
val_obs = df1[df1.TREND == 22].index.min()
val_obs

x_train1 = np.array(df1[['ADJ_EMPID']].iloc[0:val_obs])
x_train2 = np.array(df1.iloc[0:val_obs, 2:])
y_train = np.array(df1[['SRC']].iloc[0:val_obs])

x_test1 = np.array(df1[['ADJ_EMPID']].iloc[val_obs:])
x_test2 = np.array(df1.iloc[val_obs:, 2:])
y_test = np.array(df1[['SRC']].iloc[val_obs:])


# Stratified Sampling -----------------------------------#

from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing["income_cat"]):
  strat_train_set = housing.loc[train_index]
  strat_test_set = housing.loc[test_index]
  
housing["income_cat"].value_counts() / len(housing)
