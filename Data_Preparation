# Treat NA -----------------------------------------------------------#
housing.dropna(subset=["total_bedrooms"]) # Remove row with NA
housing.drop("total_bedrooms", axis=1) # Remove entire Column

housing["total_bedrooms"].fillna(median) # fill-in with median

# OR sklearn fill-in with median
from sklearn.preprocessing import Imputer
imputer = Imputer(strategy="median")
housing_num = housing.drop("ocean_proximity", axis=1) # Remove Categorical columns
imputer.fit(housing_num) # fill-in with median for all columns

imputer.statistics_ # contains medians for each column. Save this so you can apply to test dataset too
housing_num.median().values # same value as above

X = imputer.transform(housing_num) # Replace NA with medians
housing_tr = pd.DataFrame(X, columns=housing_num.columns) # convert numpy array to Pandas Dataframe


# Cateogory to Numeric  -----------------------------------------------------------#
# Option 1: Text to number
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
housing_cat = housing["ocean_proximity"]
housing_cat_encoded = encoder.fit_transform(housing_cat)
housing_cat_encoded
print(encoder.classes_) # Mapping

# Option 2: One-Hot-Encoder. Continues from Option 1
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()
housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))
housing_cat_1hot.toarray()

# Combine Option 1 and 2:
from sklearn.preprocessing import LabelBinarizer
housing_cat = housing["ocean_proximity"]
encoder = LabelBinarizer()
housing_cat_1hot = encoder.fit_transform(housing_cat)
housing_cat_1hot


# Pipeline to Combine transfomrs ----------------------------------------------#
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

from sklearn.pipeline import FeatureUnion
num_attribs = list(housing_num)
cat_attribs = ["ocean_proximity"]
num_pipeline = Pipeline([
    ('selector', DataFrameSelector(num_attribs)),
    ('imputer', Imputer(strategy="median")),
    ('attribs_adder', CombinedAttributesAdder()),
    ('std_scaler', StandardScaler()),
  ])
cat_pipeline = Pipeline([
    ('selector', DataFrameSelector(cat_attribs)),
    ('label_binarizer', LabelBinarizer()),
  ])
full_pipeline = FeatureUnion(transformer_list=[
    ("num_pipeline", num_pipeline),
    ("cat_pipeline", cat_pipeline),
  ])

housing_prepared = full_pipeline.fit_transform(housing)


# Split Train and Test Dataset -----------------------------------#
# Option 1:
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]

import numpy as np
shuffle_index = np.random.permutation(60000)
X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]

# Option 2:
np.random.seed(42)

import numpy as np
def split_train_test(data, test_ratio):
  shuffled_indices = np.random.permutation(len(data))
  test_set_size = int(len(data) * test_ratio)
  test_indices = shuffled_indices[:test_set_size]
  train_indices = shuffled_indices[test_set_size:]
  return data.iloc[train_indices], data.iloc[test_indices]
  
train_set, test_set = split_train_test(housing, 0.2)


# Corelation Feature Selection  -----------------------------------#
def corr_feature_selection(df, corr_method = 'spearman', thresh = 0.9):
    """
        Stepwise varaible selection using bivariate relationship

        Parameters:
        df (dataframe): dataframe of independent variables
        corr_method (str): 'spearman', 'pearson', 'kendall'
        thresh (demical): count corrleated varaibles with correlation >= thresh

        Returns:
        df (dataframe): Return dataframe with non-correlated varaibles
    """
    corr_num = 2
    while corr_num > 1:
        
        corrmat = df.corr(method = corr_method)
        corr_df = corrmat[abs(corrmat) >= thresh].count().sort_values(ascending = False).reset_index()
        corr_df.columns = ['variable', 'corr_var_num']
        corr_num = corr_df['corr_var_num'][0]
        
        if corr_num > 1:
            drop_col = corr_df['variable'][0]
            df = df.drop(columns=[drop_col])
        else:
            return df
            

# VIF Feature Selection  -----------------------------------#
def vif_feature_selection(df, thresh = 5):
    """
        Stepwise varaible selection using VIF

        Parameters:
        df (dataframe): dataframe of independent variables
        thresh (float): remove varaibles with VIF > = thresh

        Returns:
        df (dataframe): Return dataframe with non-correlated varaibles
    """    
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    from statsmodels.tools.tools import add_constant

    vif_value = 100
    while vif_value > thresh:
        
        df_num = df._get_numeric_data()
        numeric_col = df_num.columns
        X = add_constant(df_num)
        df_vif = pd.DataFrame([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], 
                  index=X.columns).reset_index()
        vif_value = df_vif.iloc[1,1]
        
        if vif_value > thresh:
            drop_col = df_vif.iloc[1,0]
            df = df.drop(columns=[drop_col])
        else:
            return df
            

# XGB Feature Selection  -----------------------------------#
def xgb_feature_selection(df, y, dependent_class, thresh = 5):
    """
        Stepwise varaible selection using xgb w/ permutation importance

        Parameters:
        df (dataframe): dataframe of independent variables
        y (dataframe): dataframe of dependent variables
        dependent_class (str): 'reg', 'cl'
        thresh (int): final number of variables

        Returns:
        df (dataframe): Return dataframe with important varaibles
    """
    from sklearn.inspection import permutation_importance
    
    imp_num = len(df.columns)
    while imp_num > thresh:

        if dependent_class == 'reg':
            from xgboost import XGBRegressor
            
            model = XGBRegressor().fit(df, y)
            r = permutation_importance(model, df, y, random_state = 0)
            min_imp = r.importances_mean.argmin()
            drop_col = df.columns[min_imp]
            df = df.drop(columns=[drop_col])

            imp_num = len(df.columns)
        
        if dependent_class == 'cl':
            from xgboost import XGBClassifier
            
            model = XGBClassifier().fit(df, y)
            r = permutation_importance(model, df, y, scoring = 'accuracy', random_state = 0)
            min_imp = r.importances_mean.argsort().argmin()
            drop_col = df.columns[min_imp]
            df = df.drop(columns=[drop_col])

            imp_num = len(df.columns)
            
    return df

def cross_val_rmse(model, df, y, cv = 5, shuf = False, rand_state = 1):
    """
        Calculate rmse of each cv 

        Parameters:
        model (model): e.i. RandomForestRegressor(), XGBRegressor()
        df (dataframe): dataframe of independent variables
        y (dataframe): dataframe of dependent variables
        cv (int): # of CV
        shuf (boolean) = shuffle observation
        rand_state (int) = 1 if fix shuffling observation for cv; 
                           large number(1000) if shuffling observtion for each cv run 

        Returns:
        rmse_list (list): list of rmse values from cv
    """
    # Import KFold
    from sklearn.model_selection import KFold
    from sklearn.datasets import make_classification
    from sklearn.metrics import mean_squared_error

    rmse_list = list()
    X = np.matrix(df)
    y = np.matrix(y)
    
    if shuf == False:
        cv_outer = KFold(n_splits = cv, shuffle=shuf)

        for train_ix, test_ix in cv_outer.split(X):
            X_train, X_test = X[train_ix, :], X[test_ix, :]
            y_train, y_test = y[train_ix], y[test_ix]

            model.fit(X_train, y_train)
            prediction = model.predict(X_test)
            rmse_list.append(np.sqrt(mean_squared_error(y_test, prediction)))
        return rmse_list
            
    if shuf == True:
        cv_outer = KFold(n_splits = cv, shuffle=shuf, random_state = random.randrange(rand_state))

        for train_ix, test_ix in cv_outer.split(X):
            X_train, X_test = X[train_ix, :], X[test_ix, :]
            y_train, y_test = y[train_ix], y[test_ix]

            model.fit(X_train, y_train)
            prediction = model.predict(X_test)
            rmse_list.append(np.sqrt(mean_squared_error(y_test, prediction)))
        return rmse_list
        
test = list() 

for i in range(1, len(df.columns)+1):
    print(i)
    train = xgb_feature_selection(df,y, 'reg', thresh = i)
    test.append(np.mean(cross_val_rmse(RandomForestRegressor(random_state = 0), train, y, shuf = True)))
    
best_n_features = np.array(test).argmin() + 1
xgb_feature_selection(df,y, 'reg', best_n_features).columns


# time based split -----------------------------------#

np.random.seed(42)

# Sort Data
df1 = df1.sort_values('TREND').reset_index().drop('index',axis=1)
df1.head()

# Find row where to split test dataset
val_obs = df1[df1.TREND == 22].index.min()
val_obs

x_train1 = np.array(df1[['ADJ_EMPID']].iloc[0:val_obs])
x_train2 = np.array(df1.iloc[0:val_obs, 2:])
y_train = np.array(df1[['SRC']].iloc[0:val_obs])

x_test1 = np.array(df1[['ADJ_EMPID']].iloc[val_obs:])
x_test2 = np.array(df1.iloc[val_obs:, 2:])
y_test = np.array(df1[['SRC']].iloc[val_obs:])


# Stratified Sampling -----------------------------------#

from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing["income_cat"]):
  strat_train_set = housing.loc[train_index]
  strat_test_set = housing.loc[test_index]
  
housing["income_cat"].value_counts() / len(housing)
