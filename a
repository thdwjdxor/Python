from IPython.core.display import display, HTML
display(HTML("<style>.container { width:90% !important; }</style>"))

%pylab inline

import dataiku
from dataiku import pandasutils as pdu
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy.stats import norm
from sklearn.preprocessing import StandardScaler
from scipy import stats
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

# Read the dataset as a Pandas dataframe in memory
# Note: here, we only read the first 100K rows. Other sampling options are available
df = dataiku.Dataset("2019_Market_Opportunity_joined").get_dataframe()

# Import Libraries

def corr_feature_selection(df, corr_method = 'spearman', thresh = 0.9):
    """
        Stepwise varaible selection using bivariate relationship

        Parameters:
        df (dataframe): dataframe of independent variables
        corr_method (str): 'spearman', 'pearson', 'kendall'
        thresh (demical): count corrleated varaibles with correlation >= thresh

        Returns:
        df (dataframe): Return dataframe with non-correlated varaibles
    """
    corr_num = 2
    while corr_num > 1:
        
        corrmat = df.corr(method = corr_method)
        corr_df = corrmat[abs(corrmat) >= thresh].count().sort_values(ascending = False).reset_index()
        corr_df.columns = ['variable', 'corr_var_num']
        corr_num = corr_df['corr_var_num'][0]
        
        if corr_num > 1:
            drop_col = corr_df['variable'][0]
            df = df.drop(columns=[drop_col])
        else:
            return df

def vif_feature_selection(df, thresh = 5):
    """
        Stepwise varaible selection using VIF

        Parameters:
        df (dataframe): dataframe of independent variables
        thresh (float): remove varaibles with VIF > = thresh

        Returns:
        df (dataframe): Return dataframe with non-correlated varaibles
    """    
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    from statsmodels.tools.tools import add_constant

    vif_value = 100
    while vif_value > thresh:
        
        df_num = df._get_numeric_data()
        numeric_col = df_num.columns
        X = add_constant(df_num)
        df_vif = pd.DataFrame([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], 
                  index=X.columns).reset_index()
        vif_value = df_vif.iloc[1,1]
        
        if vif_value > thresh:
            drop_col = df_vif.iloc[1,0]
            df = df.drop(columns=[drop_col])
        else:
            return df

# 1. Explore Dependent Variable: SALES_COUNT

print('Number of Unique Branches: ', len(df['Row Labels'].unique()))
df.groupby('CATEGORY')['SALES_COUNT'].count()

df['SALES_COUNT'].describe()

sns.distplot(df[df['SALES_COUNT'].notnull()]['SALES_COUNT'])

- Deviate from the normal distribution.
- Have appreciable positive skewness.
- Show peakedness

# 2. Explore Independent Variables

df_drop = df.drop(['Row Labels', 'micro_mkt'], axis=1)

df_drop = df_drop[df_drop['CATEGORY'].isin(['Total All HE Ln/Line', 'Total Business Card',
       'Total Business DDA', 'Total CD',
       'Total Consumer Card', 'Total DDA', 'Total Money Market', 'Total Personal Savings'])]

## 2.1) Correlation Matrix

#saleprice correlation matrix
corrmat = df_drop.corr()

k = 20 #number of variables for heatmap
cols = np.abs(corrmat).nlargest(k, 'SALES_COUNT')['SALES_COUNT'].index
cm = np.corrcoef(df_drop[cols].values.T)
sns.set(font_scale=1.25)
f, ax = plt.subplots(figsize=(12, 12))
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()

## 2.2) Missing Data

#missing data
total = df_drop.isnull().sum().sort_values(ascending=False)
percent = (100*df_drop.isnull().sum()/df_drop.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Missing Percent'])
missing_data.head()

# 3. Feature Selectoin

## 3.1 Featrue Selection: Bivariate Correlation

x = df_drop.drop(['SALES_COUNT'], axis = 1)
y = df_drop[['SALES_COUNT']]
x_corr = corr_feature_selection(x, thresh = 0.95)

print("Number of Featurs, Initial: ", x.shape[1])
print("Number of Features, After Correlation Filter: ", x_corr.shape[1])
print("Number of Features Removed: ", x.shape[1] - x_corr.shape[1])
print("\nFeatures Removed:")
set(set(x.columns).difference(set(x_corr.columns)))

## 3.2 Featrue Selection: VIF

x_vif = vif_feature_selection(x_corr)

print("Number of Featurs, Initial: ", x_corr.shape[1])
print("Number of Features, After Correlation Filter: ", x_vif.shape[1])
print("Number of Features Removed: ", x_corr.shape[1] - x_vif.shape[1])
print("\nFeatures Removed:")
set(set(x_corr.columns).difference(set(x_vif.columns)))

## 3.3 Feature Selection: Permuation Importance XGBCV

# Convert Categorical varibale to Dummy Variable
x_dum = pd.get_dummies(x_vif, columns=['CATEGORY'])
# x_dum = x_dum.drop(['CATEGORY_Total DDA'], axis=1)

from sklearn.inspection import permutation_importance

result = permutation_importance(XGBRegressor().fit(x_vif,y), x_vif, y, random_state = 0, scoring='neg_mean_squared_error')
for i in range(1,len(result.importances_mean)):
    print(result.importances_mean[i])

sorted_idx = result.importances_mean.argsort()
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=x_vif.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
plt.show()

# 4. Model Selection

## 4.1) Default Models

# import packages for hyperparameters tuning
from hyperopt import STATUS_OK, Trials, fmin, hp, tpe

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x_dum, y, test_size = 0.2, random_state=42)

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score

ls = LinearRegression()

np.mean(cross_val_score(ls, X_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error'))

from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

ridge = Ridge(alpha = 0.5)

np.mean(cross_val_score(ridge, X_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error'))

from xgboost import XGBRegressor
from sklearn.model_selection import cross_val_score

xgb1 = XGBRegressor()

np.mean(cross_val_score(xgb1, X_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error'))

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

rf1 = RandomForestRegressor()

np.mean(cross_val_score(rf1, X_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error'))

## 4.2) Hyperparameter Tuning Models

space={'max_depth': hp.quniform("max_depth", 3, 18, 1),
        'gamma': hp.uniform ('gamma', 1,9),
        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),
        'reg_lambda' : hp.uniform('reg_lambda', 0,1),
        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),
        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),
        'n_estimators': hp.quniform("n_estimators", 1, 100, 1),
        'booster': hp.choice('booster', ['gbtree', 'gblinear', 'dart']),
        'seed': 0
    }

from sklearn.metrics import mean_squared_error
from math import sqrt

def objective(space):
    clf=XGBRegressor(n_estimators = int(space['n_estimators']), max_depth = int(space['max_depth']), gamma = space['gamma'],
                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),
                    colsample_bytree=int(space['colsample_bytree']), booster=space['booster'])
    

    accuracy = cross_val_score(clf, X_train, y_train, cv = 3, scoring = 'neg_mean_squared_error').mean()
    print ("SCORE:", accuracy)
    return {'loss': -accuracy, 'status': STATUS_OK }

trials = Trials()

best_hyperparams = fmin(fn = objective,
                        space = space,
                        algo = tpe.suggest,
                        max_evals = 10,
                        trials = trials)

print("The best hyperparameters are : ","\n")
print(best_hyperparams)

from xgboost import XGBRegressor
from sklearn.model_selection import cross_val_score

xgb2 = XGBRegressor(colsample_bytree = 0.846060689647217, gamma = 7.350326950691216, max_depth = 11, 
                   min_child_weight = 3, n_estimators = 58, reg_alpha = 100, reg_lambda = 0.2113620198856213)

np.mean(cross_val_score(xgb2, X_train, y_train, cv = 3, scoring = 'neg_mean_absolute_error'))

space={'max_depth': hp.quniform("max_depth", 3, 18, 1),
        'n_estimators': hp.quniform("n_estimators", 1, 100, 1),
        'seed': 0
    }

from sklearn.metrics import mean_squared_error
from math import sqrt

def objective(space):
    clf=RandomForestRegressor(n_estimators = int(space['n_estimators']), max_depth = int(space['max_depth']))
    

    accuracy = cross_val_score(clf, X_train, y_train, cv = 3, scoring = 'neg_mean_squared_error').mean()
    print ("SCORE:", accuracy)
    return {'loss': -accuracy, 'status': STATUS_OK }

trials = Trials()

best_hyperparams = fmin(fn = objective,
                        space = space,
                        algo = tpe.suggest,
                        max_evals = 5,
                        trials = trials)

print("The best hyperparameters are : ","\n")
print(best_hyperparams)

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

rf2 = RandomForestRegressor(max_depth = 14, n_estimators = 43)

np.mean(cross_val_score(rf2, X_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error'))

## 4.3) Select Final Model

import math
print('LS : ', sqrt(mean_squared_error(ls.fit(X_train, y_train).predict(X_test), y_test)))
print('ridge : ', sqrt(mean_squared_error(ridge.fit(X_train, y_train).predict(X_test), y_test)))
print('xgb1 : ', sqrt(mean_squared_error(xgb1.fit(X_train, y_train).predict(X_test), y_test)))
print('xgb2 : ', sqrt(mean_squared_error(xgb2.fit(X_train, y_train).predict(X_test), y_test)))
print('rf1 : ', sqrt(mean_squared_error(rf1.fit(X_train, y_train).predict(X_test), y_test)))
print('rf2 : ', sqrt(mean_squared_error(rf2.fit(X_train, y_train).predict(X_test), y_test)))

- Final model is rf1

## 4) Shapley Value

y = a + b + c

To mearsure shapey value of 'a':
    1) predict mean(y), predict y = a, calculate difference
    2) predict y = b, predict y = a + b, calculate difference
    3) predict y = c, predict y = a + c, calculate difference
    4) predict y = b + C, predict y = a + b + c , calculate difference
    5) weighted average fo calculated differences

row_to_show = 4
data_for_prediction = X_test.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired
data_for_prediction_array = data_for_prediction.values.reshape(1, -1)

print('Actual: ', y_test.iloc[row_to_show,0])
print('Prediction: ', rf2.predict(data_for_prediction_array))

explainer

import shap  # package used to calculate Shap values

# Create object that can calculate shap values
explainer = shap.TreeExplainer(rf2)

# Calculate Shap values
shap_values = explainer.shap_values(data_for_prediction)

shap.initjs()
shap.force_plot(explainer.expected_value, shap_values, data_for_prediction)

# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.
shap_values = explainer.shap_values(X_test)

# Make plot. Index of [1] is explained in text below.
shap.summary_plot(shap_values, X_test) # , max_display = X_test.shape[1]

# make plot.
shap.dependence_plot('Transactions (Txn)', shap_values, X_test, interaction_index="Transactions (Txn)")

# make plot.
shap.dependence_plot('Transactions (Txn)', shap_values, X_test, interaction_index="CATEGORY_Total DDA")

def shap_corr(df_shap,df):
#import matplotlib as plt

# Make a copy of the input data
    shap_v = pd.DataFrame(df_shap)
    feature_list = df.columns
    shap_v.columns = feature_list
    df_v = df.copy().reset_index().drop('index',axis=1)

# Determine the correlation in order to plot with different colors: Pearson Correlatoin
    corr_list = list()
    for i in feature_list:
        b = np.corrcoef(shap_v[i],df_v[i])[1][0]
        corr_list.append(b)
    corr_df = pd.concat([pd.Series(feature_list),pd.Series(corr_list)],axis=1).fillna(0)

# Make a data frame. Column 1 is the feature, and Column 2 is the correlation coefficient
    corr_df.columns = ['Variable','Corr']
    corr_df['Sign'] = np.where(corr_df['Corr'] > 0,'red','blue')
    
# Plot it
    shap_abs = np.abs(shap_v)
    k=pd.DataFrame(shap_abs.mean()).reset_index()
    k.columns = ['Variable','SHAP_abs']
    k2 = k.merge(corr_df,left_on = 'Variable',right_on='Variable',how='inner')
    k2 = k2.sort_values(by='SHAP_abs',ascending = True)
    colorlist = k2['Sign']
    ax = k2.plot.barh(x='Variable',y='SHAP_abs',color = colorlist, fontsize = 10, figsize=(10,20),legend=False)
    ax.set_xlabel("SHAP Value (Red = Positive Impact)")

shap_corr(shap_values, X_test)






Forecasting
from IPython.core.display import display, HTML
display(HTML("<style>.container { width:90% !important; }</style>"))

%pylab inline

import dataiku
from dataiku import pandasutils as pdu
import pandas as pd
from datetime import datetime

# Read the dataset as a Pandas dataframe in memory
# Note: here, we only read the first 100K rows. Other sampling options are available
df = dataiku.Dataset("Market_Opportunity_joined").get_dataframe()

# Predict Total Monthly DDA Sales of 524 Branches, goaled in F19

## 1 Univariate: SARIMA

## 1.1 Explore Sales_Count

print('Number of Unique Branches: ', len(df['Row Labels'].unique()))
print('3 BUCs with less than 80% DDA Month Sales data')

df_prep = pd.DataFrame(df.groupby(['YEAR', 'ESSBASE_MONTH', 'CATEGORY'])['SALES_COUNT'].sum()).reset_index()

# # Forecast Specifi Branch
# dfg = df[df['CATEGORY'] == 'Total DDA'][['Row Labels', 'SALES_COUNT']].groupby('Row Labels').count()
# buc = np.array(dfg[dfg == 72].dropna().index)

# dfd = df[df['Row Labels'] == 762]

# df_prep = pd.DataFrame(dfd.groupby(['YEAR', 'ESSBASE_MONTH', 'CATEGORY'])['SALES_COUNT'].mean()).reset_index()

df_dda = df_prep[df_prep['CATEGORY'] == 'Total DDA']
df_dda['Date'] = df_dda.apply(lambda row: datetime(row['YEAR'], row['ESSBASE_MONTH'], 1), axis=1)
df_dda = df_dda[['Date', 'SALES_COUNT']]
df_dda.set_index('Date', inplace=True)

df_dda.head()

plt.figure(figsize=(30,5))
plt.plot(df_dda)

import statsmodels.api as sm  
from statsmodels.tsa.seasonal import seasonal_decompose
result = seasonal_decompose(df_dda, model='multiplicative')
fig = result.plot()
fig.set_size_inches(12, 6);

- Show Upper Trend
- Show Seasonality

## 1.1.1 Stationarity: Sales_Count

### Note: traditional time-series modeling require stationarity of variable

from statsmodels.tsa.stattools import adfuller

#Perform Augmented Dickey–Fuller test:
print('Results of Dickey Fuller Test:')
dftest = adfuller(df_dda['SALES_COUNT'], autolag='AIC')

dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
    
print(dfoutput)

- ADF Test fail to reject the null that Sales_Count is not-stationary. Therefore, necessary transomartion is needed(e.i. log, first-difference, moving-average difference,etc)

## 1.1.2 Stationarity: log(Sales_Count)

df_dda_log = np.log(df_dda)
plt.figure(figsize=(30,5))
plt.plot(df_dda_log)

from statsmodels.tsa.stattools import adfuller

#Perform Augmented Dickey–Fuller test:
print('Results of Dickey Fuller Test:')
dftest = adfuller(df_dda_log['SALES_COUNT'], autolag='AIC')

dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
    
print(dfoutput)

- ADF Test fail to reject the null that log(Sales_Count) is not-stationary.

## 1.1.3 Stationarity: diff(Sales_Count)

df_dda_diff = df_dda.diff().dropna()
plt.figure(figsize=(30,5))
plt.plot(df_dda_diff)

from statsmodels.tsa.stattools import adfuller

#Perform Augmented Dickey–Fuller test:
print('Results of Dickey Fuller Test:')
dftest = adfuller(df_dda_diff['SALES_COUNT'], autolag='AIC')

dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
    
print(dfoutput)

- ADF Test fail to reject the null that diff(Sales_Count) is not-stationary.

## 1.1.4 Stationarity: diff(Sales_Count).Seasonality

df_dda_diff_season = df_dda.diff(12).dropna()
plt.figure(figsize=(30,5))
plt.plot(df_dda_diff_season)

from statsmodels.tsa.stattools import adfuller

#Perform Augmented Dickey–Fuller test:
print('Results of Dickey Fuller Test:')
dftest = adfuller(df_dda_diff_season['SALES_COUNT'], autolag='AIC')

dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
    
print(dfoutput)

- ADF Test fail to reject the null that diff(Sales_Count).season is not-stationary.

## 1.1.5 Stationarity: diff(Sales_Count).Seasonality.diff()

df_dda_diff_season = df_dda.diff(12).dropna().diff().dropna()
plt.figure(figsize=(30,5))
plt.plot(df_dda_diff_season)

from statsmodels.tsa.stattools import adfuller

#Perform Augmented Dickey–Fuller test:
print('Results of Dickey Fuller Test:')
dftest = adfuller(df_dda_diff_season['SALES_COUNT'], autolag='AIC')

dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
    
print(dfoutput)

- ADF Test reject the null that diff(Sales_Count).season is not-stationary.

from statsmodels.tsa.stattools import acf, pacf

#ACF & PACF plots

lag_acf = acf(df_dda_diff_season, nlags=30)
lag_pacf = pacf(df_dda_diff_season, nlags=30, method='ols')

#Plot ACF:
plt.figure(figsize=(20,5))
plt.subplot(121)
plt.plot(lag_acf, marker='o')
plt.axhline(y=0, linestyle='--', color='gray')
plt.axhline(y=-1.96/np.sqrt(len(df_dda_diff_season)), linestyle='--', color='gray')
plt.axhline(y=1.96/np.sqrt(len(df_dda_diff_season)), linestyle='--', color='gray')
plt.title('Autocorrelation Function')            

#Plot PACF
plt.subplot(122)
plt.plot(lag_pacf, marker='o')
plt.axhline(y=0, linestyle='--', color='gray')
plt.axhline(y=-1.96/np.sqrt(len(df_dda_diff_season)), linestyle='--', color='gray')
plt.axhline(y=1.96/np.sqrt(len(df_dda_diff_season)), linestyle='--', color='gray')
plt.title('Partial Autocorrelation Function')
            
plt.tight_layout() 

# 2.1 Seasonality + Linear Forecast

### Sales -> remove Monthly Seasonality -> 

n_test = 12
yhat = []
for i in range(n_test, 0, -1):
    df1 = df_dda.copy()
    df1['month'] = df1.index.month
    df2 = df1[:-i]
    mont_avg = pd.DataFrame(df2.groupby(['month'])['SALES_COUNT'].mean()/ df2['SALES_COUNT'].mean()).reset_index() 
    df2 = df2.merge(mont_avg, how = 'left', on = 'month')
    df2['ds_sales'] = df2['SALES_COUNT_x'] / df2['SALES_COUNT_y']

    X = pd.Series(np.arange(len(df2)))
    y = df2['ds_sales']
    X = sm.add_constant(X)
    model = sm.OLS(y,X, missing='drop').fit()
    
    yhat.append(((len(df2) + 1) * model.params[0] + model.params['const']) * float(df2.tail(12)[:1]['SALES_COUNT_y']))

from sklearn.metrics import mean_squared_error
# evaluate forecasts
rmse = np.sqrt(mean_squared_error(yhat, list(df_dda.iloc[-n_test:,0])))
print('Test RMSE: %.3f' % rmse)
# plot forecasts against actual outcomes
pyplot.plot(list(df_dda.iloc[-n_test:,0]))
pyplot.plot(yhat, color='red')
pyplot.show()

# 2.2 SARIMAX

#SOURCE: https://machinelearningmastery.com/how-to-grid-search-sarima-model-hyperparameters-for-time-series-forecasting-in-python/
# grid search sarima hyperparameters
from math import sqrt
from multiprocessing import cpu_count
from joblib import Parallel
from joblib import delayed
from warnings import catch_warnings
from warnings import filterwarnings
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error

# one-step sarima forecast
def sarima_forecast(history, config):
    order, sorder, trend = config
    # define model
    model = SARIMAX(history, order=order, seasonal_order=sorder, trend=trend, enforce_stationarity=False, enforce_invertibility=False)
#     model = SARIMAX(history, order=order, seasonal_order=sorder)
    # fit model
    model_fit = model.fit(disp=False)
    # make one step forecast
    yhat = model_fit.predict(len(history), len(history))
    return yhat[0]

# root mean squared error or rmse
def measure_rmse(actual, predicted):
    return sqrt(mean_squared_error(actual, predicted))

# split a univariate dataset into train/test sets
def train_test_split(data, n_test):
    return data[:-n_test], data[-n_test:]

# walk-forward validation for univariate data
def walk_forward_validation(data, n_test, cfg):
    predictions = list()
    # split dataset
    train, test = train_test_split(data, n_test)
    # seed history with training dataset
    history = [x for x in train]
    # step over each time-step in the test set
    for i in range(len(test)):
        # fit model and make forecast for history
        yhat = sarima_forecast(history, cfg)
        # store forecast in list of predictions
        predictions.append(yhat)
        # add actual observation to history for the next loop
        history.append(test[i])
    # estimate prediction error
    error = measure_rmse(test, predictions)
    return error

# score a model, return None on failure
def score_model(data, n_test, cfg, debug=False):
    result = None
    # convert config to a key
    key = str(cfg)
    # show all warnings and fail on exception if debugging
    if debug:
        result = walk_forward_validation(data, n_test, cfg)
    else:
        # one failure during model validation suggests an unstable config
        try:
            # never show warnings when grid searching, too noisy
            with catch_warnings():
                filterwarnings("ignore")
                result = walk_forward_validation(data, n_test, cfg)
        except:
            error = None
    # check for an interesting result
    if result is not None:
        print(' > Model[%s] %.3f' % (key, result))
    return (key, result)

# grid search configs
def grid_search(data, cfg_list, n_test, parallel=True):
    scores = None
    if parallel:
        # execute configs in parallel
        executor = Parallel(n_jobs=cpu_count(), backend='multiprocessing')
        tasks = (delayed(score_model)(data, n_test, cfg) for cfg in cfg_list)
        scores = executor(tasks)
    else:
        scores = [score_model(data, n_test, cfg) for cfg in cfg_list]
    # remove empty results
    scores = [r for r in scores if r[1] != None]
    # sort configs by error, asc
    scores.sort(key=lambda tup: tup[1])
    return scores

# create a set of sarima configs to try
def sarima_configs(seasonal=[12]):
    models = list()
    # define config lists
    p_params = [0, 1, 2]
    d_params = [0, 1]
    q_params = [0, 1, 2]
    t_params = ['c','t','ct']
    P_params = [0, 1, 2]
    D_params = [0, 1]
    Q_params = [0, 1, 2]
    m_params = seasonal
    # create config instances
    for p in p_params:
        for d in d_params:
            for q in q_params:
                for t in t_params:
                    for P in P_params:
                        for D in D_params:
                            for Q in Q_params:
                                for m in m_params:
                                    cfg = [(p,d,q), (P,D,Q,m), t]
                                    models.append(cfg)
    return models

if __name__ == '__main__':
    # define dataset
    data = df_dda.iloc[:,0]
    print(data)
    # data split
    n_test = 12
    # model configs
    cfg_list = sarima_configs()
    # grid search
    scores = grid_search(data, cfg_list, n_test)
    print('done')
    # list top 3 configs
    for cfg, error in scores[:3]:
        print(cfg, error)

# grid search sarima hyperparameters
from math import sqrt
from multiprocessing import cpu_count
from joblib import Parallel
from joblib import delayed
from warnings import catch_warnings
from warnings import filterwarnings
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error

# load dataset
series = df_dda.iloc[:,0]
# split into train and test sets
X = series.values
train, test = X[0:-n_test], X[-n_test:]
history = list(train)
predictions = list()

# walk-forward validation
for t in range(len(test)):
    # fit model
    model = SARIMAX(history, order=[0,0,0], seasonal_order=[0,1,1,12], trend='ct', enforce_stationarity=False, enforce_invertibility=False)
    model_fit = model.fit()
    # one step forecast
    yhat = model_fit.predict(len(history), len(history))
    # store forecast and ob
    predictions.append(yhat)
    history.append(test[t])
    
# evaluate forecasts
rmse = sqrt(mean_squared_error(test, predictions))
print('Test RMSE: %.3f' % rmse)
# plot forecasts against actual outcomes
pyplot.plot(test)
pyplot.plot(predictions, color='red')
pyplot.show()

print(model_fit.summary())

# 2.3 Univeriate: Simple ES

def walk_forward_validation(data, n_test):
    predictions = list()
    # split dataset
    train, test = train_test_split(data, n_test)
    # seed history with training dataset
    history = [x for x in train]
    # step over each time-step in the test set
    for i in range(len(test)):
        # fit model and make forecast for history
        model = SimpleExpSmoothing(history)
        model_fit = model.fit()
        yhat = model_fit.predict(len(data), len(data))
        # store forecast in list of predictions
        predictions.append(yhat)
        # add actual observation to history for the next loop
        history.append(test[i])
    # estimate prediction error
    error = measure_rmse(test, predictions)
    return error

# SES example
from statsmodels.tsa.holtwinters import SimpleExpSmoothing
from random import random
# contrived dataset
data = df_dda.iloc[:,0]

walk_forward_validation(data, 12)

# load dataset
series = df_dda.iloc[:,0]
# split into train and test sets
X = series.values
train, test = X[0:-12], X[-12:]
history = list(train)
predictions = list()

# walk-forward validation
for t in range(len(test)):
    # fit model
    model = SimpleExpSmoothing(history)
    model_fit = model.fit()
    yhat = model_fit.predict(len(history), len(history))
    # store forecast and ob
    predictions.append(yhat)
    history.append(test[t])
    
# evaluate forecasts
rmse = sqrt(mean_squared_error(test, predictions))
print('Test RMSE: %.3f' % rmse)
# plot forecasts against actual outcomes
pyplot.plot(test)
pyplot.plot(predictions, color='red')
pyplot.show()

# 2.4 Univeriate: Holt Winter’s Exponential Smoothing (HWES)

# grid search holt winter's exponential smoothing
from math import sqrt
from multiprocessing import cpu_count
from joblib import Parallel
from joblib import delayed
from warnings import catch_warnings
from warnings import filterwarnings
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.metrics import mean_squared_error
from numpy import array
 
# one-step Holt Winter’s Exponential Smoothing forecast
def exp_smoothing_forecast(history, config):
	t,d,s,p,b,r = config
	# define model
	history = array(history)
	model = ExponentialSmoothing(history, trend=t, damped=d, seasonal=s, seasonal_periods=p)
	# fit model
	model_fit = model.fit(optimized=True, use_boxcox=b, remove_bias=r)
	# make one step forecast
	yhat = model_fit.predict(len(history), len(history))
	return yhat[0]
 
# root mean squared error or rmse
def measure_rmse(actual, predicted):
	return sqrt(mean_squared_error(actual, predicted))
 
# split a univariate dataset into train/test sets
def train_test_split(data, n_test):
	return data[:-n_test], data[-n_test:]
 
# walk-forward validation for univariate data
def walk_forward_validation(data, n_test, cfg):
	predictions = list()
	# split dataset
	train, test = train_test_split(data, n_test)
	# seed history with training dataset
	history = [x for x in train]
	# step over each time-step in the test set
	for i in range(len(test)):
		# fit model and make forecast for history
		yhat = exp_smoothing_forecast(history, cfg)
		# store forecast in list of predictions
		predictions.append(yhat)
		# add actual observation to history for the next loop
		history.append(test[i])
	# estimate prediction error
	error = measure_rmse(test, predictions)
	return error
 
# score a model, return None on failure
def score_model(data, n_test, cfg, debug=False):
	result = None
	# convert config to a key
	key = str(cfg)
	# show all warnings and fail on exception if debugging
	if debug:
		result = walk_forward_validation(data, n_test, cfg)
	else:
		# one failure during model validation suggests an unstable config
		try:
			# never show warnings when grid searching, too noisy
			with catch_warnings():
				filterwarnings("ignore")
				result = walk_forward_validation(data, n_test, cfg)
		except:
			error = None
	# check for an interesting result
	if result is not None:
		print(' > Model[%s] %.3f' % (key, result))
	return (key, result)
 
# grid search configs
def grid_search(data, cfg_list, n_test, parallel=True):
	scores = None
	if parallel:
		# execute configs in parallel
		executor = Parallel(n_jobs=cpu_count(), backend='multiprocessing')
		tasks = (delayed(score_model)(data, n_test, cfg) for cfg in cfg_list)
		scores = executor(tasks)
	else:
		scores = [score_model(data, n_test, cfg) for cfg in cfg_list]
	# remove empty results
	scores = [r for r in scores if r[1] != None]
	# sort configs by error, asc
	scores.sort(key=lambda tup: tup[1])
	return scores
 
# create a set of exponential smoothing configs to try
def exp_smoothing_configs(seasonal=[None]):
	models = list()
	# define config lists
	t_params = ['add', 'mul', None]
	d_params = [True, False]
	s_params = ['add', 'mul', None]
	p_params = seasonal
	b_params = [True, False]
	r_params = [True, False]
	# create config instances
	for t in t_params:
		for d in d_params:
			for s in s_params:
				for p in p_params:
					for b in b_params:
						for r in r_params:
							cfg = [t,d,s,p,b,r]
							models.append(cfg)
	return models
 
if __name__ == '__main__':
	# define dataset
	data = df_dda.iloc[:,0]
	# data split
	n_test = 12
	# model configs
	cfg_list = exp_smoothing_configs()
	# grid search
	scores = grid_search(data, cfg_list, n_test)
	print('done')
	# list top 3 configs
	for cfg, error in scores[:3]:
		print(cfg, error)

# load dataset
series = df_dda.iloc[:,0]
# split into train and test sets
X = series.values
train, test = X[0:-12], X[-12:]
history = list(train)
predictions = list()

# walk-forward validation
for t in range(len(test)):
    # fit model
    model = ExponentialSmoothing(data,  trend='add', damped=True, seasonal=None, seasonal_periods=None)
    model_fit = model.fit(optimized=True, use_boxcox=False, remove_bias=False)
    # make prediction
    yhat = model_fit.predict(len(history), len(history))
    # store forecast and ob
    predictions.append(yhat)
    history.append(test[t])
    
# evaluate forecasts
rmse = sqrt(mean_squared_error(test, predictions))
print('Test RMSE: %.3f' % rmse)
# plot forecasts against actual outcomes
pyplot.plot(test)
pyplot.plot(predictions, color='red')
pyplot.show()

# 2.5 Univeriate: Supervised Learning

from numpy import array

# split a univariate sequence into samples
def split_sequence(sequence, n_steps):
    X, y = list(), list()
    for i in range(len(sequence)):
        # find the end of this pattern
        end_ix = i + n_steps
        # check if we are beyond the sequence
        if end_ix > len(sequence)-1:
            break
        # gather input and output parts of the pattern
        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
        X.append(seq_x)
        y.append(seq_y)
    return array(X), array(y)

# define input sequence
raw_seq = df_dda.iloc[:,0]
# choose a number of time steps
n_steps = 12
# split into samples
X, y = split_sequence(raw_seq, n_steps)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.23, shuffle = False)

from sklearn.linear_model import LinearRegression
ls = LinearRegression()
ls.fit(X_train, y_train)
print('Linear Regresion: ', np.sqrt(mean_squared_error(ls.predict(X_test),y_test)))

# from sklearn.linear_model import Ridge
# ridge = Ridge(alpha = 0.9)
# ridge.fit(X_train, y_train)
# print('Ridge: ',np.sqrt(mean_squared_error(ridge.predict(X_test),y_test)))

from xgboost import XGBRegressor
xgb = XGBRegressor()
xgb.fit(X_train, y_train)
print('xgb: ', np.sqrt(mean_squared_error(xgb.predict(X_test),y_test)))

from sklearn.ensemble import RandomForestRegressor
rf1 = RandomForestRegressor()
rf1.fit(X_train, y_train)
print('rf: ', np.sqrt(mean_squared_error(rf1.predict(X_test),y_test)))

rmse = sqrt(mean_squared_error(xgb.predict(X_test),y_test))
print('Test RMSE: %.3f' % rmse)

pyplot.plot(y_test)
pyplot.plot(xgb.predict(X_test), color='red')
pyplot.show()




# smote
%pylab inline

import dataiku
from dataiku import pandasutils as pdu
import pandas as pd
pd.set_option('display.max_rows', 10000)
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams
import seaborn as sns
from scipy import stats
sns.set_style('whitegrid')
import warnings
warnings.filterwarnings('ignore')

# Read the dataset as a Pandas dataframe in memory
# Note: here, we only read the first 100K rows. Other sampling options are available
df = dataiku.Dataset("SMOTE_DATA").get_dataframe()

# Get some simple descriptive statistics
pdu.audit(df)

print('Original Data Shape: ', df.shape)

df['int_rate'] = df['int_rate'].str.replace('%','').astype('float')

df.head()

df['loan_status'].value_counts()

df1=df.copy()

df1.loc[(df1["loan_status"]=='Fully Paid') | (df1["loan_status"]=='Does not meet the credit policy. Status:Fully Paid'),'target'] = 'Fully Paid'
df1.loc[(df1["loan_status"]=='Charged Off') | (df1["loan_status"]=='Does not meet the credit policy. Status:Charged Off'),'target'] = 'Charged-Off'

#check new target variable
df1.target.value_counts()

Target varialbe is imbalanced

df1.drop('loan_status',axis=1,inplace=True)

df1['mths_since_last_delinq'].fillna(value=0,inplace=True)

df1.isnull().sum()/len(df1)*100

df2 = df1.drop(columns=['issue_d'])

df2['log_annual_inc'] = df2['annual_inc'].apply(lambda x: np.log10(x+1))
df2.drop('annual_inc', axis=1, inplace=True)
df2['log_annual_inc'].describe()

df2.info()

df3 = df2[df2['earliest_cr_line'].notna()]

df3['earliest_cr_line'] = df3['earliest_cr_line'].str.strip().str[-4:].astype(int)

df3['earliest_cr_line'].describe()

df3.info()

df3.head()

#Convert to dummy variables
df4 = pd.get_dummies(df3, columns=['term','home_ownership','verification_status','purpose','addr_state'], drop_first=True)

# Base LOGIT

from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df4.drop('target',axis=1),df4['target'],test_size=0.2,random_state=123)

y_train.value_counts()

y_test.value_counts()

logreg = LogisticRegression()
logreg.fit(X_train, y_train)

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred)
print(confusion_matrix)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

# Undersampling

from imblearn.under_sampling import RandomUnderSampler

# define undersample strategy
undersample = RandomUnderSampler(sampling_strategy=1)

# fit and apply the transform
X_over, y_over = undersample.fit_resample(X_train, y_train)

y_over.shape

logreg_us = LogisticRegression()
logreg_us.fit(X_over, y_over)

y_pred_us = logreg_us.predict(X_test)

from sklearn.metrics import confusion_matrix
confusion_matrix_us = confusion_matrix(y_test, y_pred_us)
print(confusion_matrix_us)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_us))

# SMOTE

#Oversampling only the training set using Synthetic Minority Oversampling Technique (SMOTE)
from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=123, ratio = 1.0)
x_train_r, y_train_r = sm.fit_sample(X_train, y_train)

d={'status':y_train_r}
train_sm =pd.DataFrame(d)

train_sm.status.value_counts()

logreg_sm = LogisticRegression()
logreg_sm.fit(x_train_r, y_train_r)

y_pred_sm = logreg_sm.predict(X_test)

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred_sm)
print(confusion_matrix)

print(classification_report(y_test, y_pred_sm))

# SMOTENC

from imblearn.over_sampling import SMOTENC

X_train_nc, X_test_nc, y_train_nc, y_test_nc = train_test_split(df3.drop('target',axis=1),df3['target'],test_size=0.2,random_state=123)

smote_nc = SMOTENC(categorical_features=[1,3,4,5,6], random_state=0)
X_resampled, y_resampled = smote_nc.fit_resample(X_train_nc, y_train_nc)

len(X_resampled)

X_train_nc.shape

df_nc = pd.DataFrame(data=X_resampled[0:,0:])

df_nc.columns = X_train_nc.columns

df_nc.head()

#Convert to dummy variables
X_train_nc1 = pd.get_dummies(df_nc, columns=['term','home_ownership','verification_status','purpose','addr_state'], drop_first=True)

d={'status':y_resampled}
y_train_smnc =pd.DataFrame(d)

y_train_smnc.status.value_counts()

logreg_smnc = LogisticRegression()
logreg_smnc.fit(X_train_nc1, y_train_smnc)

y_pred_smnc = logreg_smnc.predict(X_test)
print('Accuracy of SMOTENC logistic regression classifier on test set: {:.2f}'.format(logreg_smnc.score(X_test, y_test)))

from sklearn.metrics import confusion_matrix
confusion_matrix_smnc = confusion_matrix(y_test, y_pred_smnc)
print(confusion_matrix_smnc)

print(classification_report(y_test, y_pred_smnc))
