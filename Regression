# Linear Regression ---------------------------------------------- #
from sklearn.linear_model import LinearRegression
housing_predictions = np.c_[country_stats["GDP per capita"]]
housing_labels = np.c_[country_stats["Life satisfaction"]]

# Select a linear model
lin_reg = sklearn.linear_model.LinearRegression()

# Train the model
lin_reg.fit(housing_predictions, housing_labels)

# Make a prediction for Cyprus
some_x_test = housing.iloc[:5]
some_y_test = housing_labels.iloc[:5]
some_data_prepared = full_pipeline.transform(some_x_train)
print("Predictions:\t", lin_reg.predict(some_data_prepared)) # Predicted
print("Labels:\t\t", list(some_labels)) # Acutal 

# in-sample RMSE
from sklearn.metrics import mean_squared_error
housing_predictions = lin_reg.predict(housing_prepared)
lin_mse = mean_squared_error(housing_labels, housing_predictions)
lin_rmse = np.sqrt(lin_mse)
lin_rmse

# CV out-of-sample RMSE
from sklearn.model_selection import cross_val_score
scores = cross_val_score(lin_reg, housing_prepared, housing_labels,
scoring="neg_mean_squared_error", cv=10)
rmse_scores = np.sqrt(-scores)

# show CV results
def display_scores(scores):
  print("Scores:", scores)
  print("Mean:", scores.mean())
  print("Standard deviation:", scores.std())
display_scores(rmse_scores)

# Save model, Hyperparameter, CV scores, actual prediction
from sklearn.externals import joblib
joblib.dump(my_model, "my_model.pkl")
# and later...
my_model_loaded = joblib.load("my_model.pkl")


# Decision Tree ---------------------------------------------- #
from sklearn.tree import DecisionTreeRegressor
tree_reg = DecisionTreeRegressor()
tree_reg.fit(housing_prepared, housing_labels)

# in-sample RMSE
housing_predictions = tree_reg.predict(housing_prepared)
tree_mse = mean_squared_error(housing_labels, housing_predictions)
tree_rmse = np.sqrt(tree_mse)

# CV out-of-sample RMSE
from sklearn.model_selection import cross_val_score
scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
scoring="neg_mean_squared_error", cv=10)
rmse_scores = np.sqrt(-scores)

# show CV results
def display_scores(scores):
  print("Scores:", scores)
  print("Mean:", scores.mean())
  print("Standard deviation:", scores.std())
display_scores(tree_rmse_scores)


# RandomForest Tree ---------------------------------------------- #
from sklearn.ensemble import RandomForestRegressor
forest_reg = RandomForestRegressor()
forest_reg.fit(housing_prepared, housing_labels)


# kNN Regression ---------------------------------------------- #
clf = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)


# Hyper Parameter Tunning -------------------------------------------#
from sklearn.model_selection import GridSearchCV
param_grid = [
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, #3 x 4 = 12 models
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}, # 2 * 3 = 6 models
  ] # total of 12 + 6 = 18 models
  
forest_reg = RandomForestRegressor()
grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(housing_prepared, housing_labels)

grid_search.best_params_ # selected parameters
grid_search.best_estimator_ # more granular parameters

# show scoresfor all parameters
cvres = grid_search.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
  print(np.sqrt(-mean_score), params)
  
  
# Variable Importance -----------------------------------------------------------#
feature_importances = grid_search.best_estimator_.feature_importances_

extra_attribs = ["rooms_per_hhold", "pop_per_hhold", "bedrooms_per_room"]
cat_one_hot_attribs = list(encoder.classes_)
attributes = num_attribs + extra_attribs + cat_one_hot_attribs


# Final Model  -----------------------------------------------------------#
final_model = grid_search.best_estimator_ # get the best parameter

X_test_prepared = full_pipeline.transform(X_test)

final_predictions = final_model.predict(X_test_prepared)
final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse) # => evaluates to 48,209.6
sorted(zip(feature_importances, attributes), reverse=True)
