# 1. Import Libraries 

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline

# 2. Prepare Data

df = pd.read_csv("data/cereal.csv")
df_drop = df.drop(['ID', 'REGION'], axis=1)

#missing data
total = df_drop.isnull().sum().sort_values(ascending=False)
percent = (100*df_drop.isnull().sum()/df_drop.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Missing Percent'])
missing_data.head()

# 3. Feature Selection: PCA

pca_percent = 0.8
pipeline = Pipeline([('scaling', MinMaxScaler()), ('pca', PCA(pca_percent))])
df_pca = pd.DataFrame(pipeline.fit_transform(df_corr))
df_pca.shape

# 4. Clustering Algorihtms

## 4.1 K-means

from sklearn.cluster import KMeans
wss = []
k_range = 10

for k in range(1, k_range):
    kmeans = KMeans(n_clusters=k, random_state=1).fit(df_pca)
    wss.append(np.sqrt(kmeans.inertia_))
    
plt.figure(figsize=(10,10))
plt.plot(range(1, k_range), wss, marker='s')
plt.xlabel('$k$')
plt.ylabel('$J(C_k)$');

from sklearn.cluster import KMeans
sil = []
k_range = 10

for k in range(2, k_range):
    kmeans = KMeans(n_clusters=k, random_state=1).fit(df_pca)
    sil.append(metrics.silhouette_score(df_pca, kmeans.labels_))
    
plt.figure(figsize=(10,10))
plt.plot(range(2, k_range), sil, marker='s')
plt.xlabel('# of Clusters')
plt.ylabel('Silhouette');

## 4.2 AffinityPropagation

# affinity propagation clustering
from sklearn.cluster import AffinityPropagation

sil = []
damp = np.arange(0.5, 0.8, 0.01)

for i in damp:
    aff_prop = AffinityPropagation(damping = i).fit(df_pca)
    sil.append(metrics.silhouette_score(df_pca, aff_prop.labels_))
    
plt.figure(figsize=(10,10))
plt.plot(damp, sil, marker='s')
plt.xlabel('Damp')
plt.ylabel('Silhouette');

## 4.3 Agglomerative

from sklearn.cluster import KMeans
sil = []
k_range = 10

for k in range(2, k_range):
    agg_clust = AgglomerativeClustering(n_clusters=k).fit(df_pca)
    sil.append(metrics.silhouette_score(df_pca, agg_clust.labels_))
    
plt.figure(figsize=(10,10))
plt.plot(range(2, k_range), sil, marker='s')
plt.xlabel('# of Clusters')
plt.ylabel('Silhouette');

## 4.4 BIRCH

from sklearn.cluster import Birch

sil_dict = {}
sil_list = []
k_range = 10
thresh = np.arange(0.5, 1, 0.5)


for i in thresh:
    for k in range(2, k_range):
        birch = Birch(threshold = i, n_clusters=k).fit(df_pca)
        sil_dict[k] = metrics.silhouette_score(df_pca, birch.labels_)
    sil_list.append(sil_dict)
    inertia = {}
    
sil_df = pd.DataFrame(sil_list, index = thresh)
sil_df.describe()

# plt.figure(figsize=(10,10))
# plt.plot(range(2, k_range), inertia, marker='s')
# plt.xlabel('# of Clusters')
# plt.ylabel('Silhouette');
# It seems that this algo doesn’t offer many advantages over K-means, unless your dataset is really big and doesn’t fit in memory.

## 4.5 DBSCAN

from sklearn.cluster import DBSCAN
from itertools import product

eps_values = np.arange(2, 5, 0.25) # eps values to be investigated
min_samples = np.arange(3, 10) # min_samples values to be investigated
DBSCAN_params = list(product(eps_values, min_samples))

no_of_clusters = []
sil_score = []

for p in DBSCAN_params:
    DBS_clustering = DBSCAN(eps=p[0], min_samples=p[1]).fit(df_pca)
    no_of_clusters.append(len(np.unique(DBS_clustering.labels_)))
    sil_score.append(metrics.silhouette_score(df_pca, DBS_clustering.labels_))

tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   
tmp['No_of_clusters'] = no_of_clusters

pivot_1 = pd.pivot_table(tmp, values='No_of_clusters', index='Min_samples', columns='Eps')

fig, ax = plt.subplots(figsize=(12,6))
sns.heatmap(pivot_1, annot=True,annot_kws={"size": 16}, cmap="YlGnBu", ax=ax)
ax.set_title('Number of clusters')
plt.show()

tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   
tmp['Sil_score'] = sil_score

pivot_1 = pd.pivot_table(tmp, values='Sil_score', index='Min_samples', columns='Eps')

fig, ax = plt.subplots(figsize=(18,6))
sns.heatmap(pivot_1, annot=True, annot_kws={"size": 10}, cmap="YlGnBu", ax=ax)
plt.show()

## 4.6 Gauss-Mixture

from sklearn.mixture import GaussianMixture

sil = []
k_range = 4

for k in range(2, k_range):
    gauss = GaussianMixture(n_components=k).fit(df_pca)
    sil.append(metrics.silhouette_score(df_pca, gauss.predict(df_pca)))
    
plt.figure(figsize=(10,10))
plt.plot(range(2, k_range), sil, marker='s')
plt.xlabel('# of Clusters')
plt.ylabel('Silhouette');

## 4.7 Summary

from sklearn import metrics
from sklearn import datasets
import pandas as pd
from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation, SpectralClustering


data = datasets.load_digits()
X, y = data.data, data.target

algorithms = []
algorithms.append(KMeans(n_clusters=4, random_state=1))
algorithms.append(AgglomerativeClustering(n_clusters=4))

data = []
for algo in algorithms:
    algo.fit(X)
    data.append(({
        'ARI': metrics.adjusted_rand_score(y, algo.labels_),
        'AMI': metrics.adjusted_mutual_info_score(y, algo.labels_,
                                                 average_method='arithmetic'),
        'Homogenity': metrics.homogeneity_score(y, algo.labels_),
        'Completeness': metrics.completeness_score(y, algo.labels_),
        'V-measure': metrics.v_measure_score(y, algo.labels_),
        'Silhouette': metrics.silhouette_score(X, algo.labels_)}))
    print(data)

results = pd.DataFrame(data=data, columns=['ARI', 'AMI', 'Homogenity',
                                           'Completeness', 'V-measure', 
                                           'Silhouette'],
                       index=['K-means', 'Agglomerative'])

results
