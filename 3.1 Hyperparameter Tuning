# SPLIT Data: Train/Split-----------------------------------------------------------------#
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x_dum, y, test_size = 0.2, random_state=42)


# Train Multiple Models: Default -----------------------------------------------------------------#
from sklearn.model_selection import cross_val_score

from sklearn.linear_model import LinearRegression
ls = LinearRegression()
np.mean(cross_val_score(ls, X_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error'))
ls.fit(X_train, y_train)

from sklearn.linear_model import Ridge
ridge = Ridge(alpha = 0.5)
np.mean(cross_val_score(ridge, X_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error'))
ridge.fit(X_train, y_train)

from xgboost import XGBRegressor
xgb1 = XGBRegressor()
np.mean(cross_val_score(xgb, X_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error'))
xgb1.fit(X_train, y_train)

from sklearn.ensemble import RandomForestRegressor
rf1 = RandomForestRegressor()
np.mean(cross_val_score(rf, X_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error'))
rf1.fit(X_train, y_train)


# Train Multiple Models: Hyperparameter Tuning -----------------------------------------------------------------#
from math import sqrt
from xgboost import XGBRegressor
from sklearn.model_selection import cross_val_score

# XGB Hyperparameter--------#

def objective(space):
    clf=XGBRegressor(n_estimators = int(space['n_estimators']), max_depth = int(space['max_depth']), gamma = space['gamma'],
                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),
                    colsample_bytree=int(space['colsample_bytree']), booster=space['booster'])
    
    accuracy = cross_val_score(clf, X_train, y_train, cv = 3, scoring = 'neg_mean_squared_error').mean()
    print ("SCORE:", accuracy)
    return {'loss': -accuracy, 'status': STATUS_OK }

space={'max_depth': hp.quniform("max_depth", 3, 18, 1),
        'gamma': hp.uniform ('gamma', 1,9),
        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),
        'reg_lambda' : hp.uniform('reg_lambda', 0,1),
        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),
        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),
        'n_estimators': hp.quniform("n_estimators", 1, 100, 1),
        'booster': hp.choice('booster', ['gbtree', 'gblinear', 'dart']),
        'seed': 0
    }

trials = Trials()
best_hyperparams = fmin(fn = objective,
                        space = space,
                        algo = tpe.suggest,
                        max_evals = 100,
                        trials = trials)

print("The best hyperparameters are : ","\n")
print(best_hyperparams)

xgb2 = XGBRegressor(colsample_bytree = 0.8118361566670154, gamma = 2.1844503527016754, max_depth = 3, 
                   min_child_weight = 5, n_estimators = 77, reg_alpha = 66, reg_lambda = 0.0696850076667016)
np.mean(cross_val_score(xgb1, X_train, y_train, cv = 3, scoring = 'neg_mean_absolute_error'))
xgb2.fit(X_train, y_train)


# RF Hyperparameter-----------#
from math import sqrt
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

def objective(space):
    clf=RandomForestRegressor(n_estimators = int(space['n_estimators']), max_depth = int(space['max_depth']))
    
    accuracy = cross_val_score(clf, X_train, y_train, cv = 3, scoring = 'neg_mean_squared_error').mean()
    print ("SCORE:", accuracy)
    return {'loss': -accuracy, 'status': STATUS_OK }

space={'max_depth': hp.quniform("max_depth", 3, 18, 1),
        'n_estimators': hp.quniform("n_estimators", 1, 100, 1),
        'seed': 0
    }
   
trials = Trials()
best_hyperparams = fmin(fn = objective,
                        space = space,
                        algo = tpe.suggest,
                        max_evals = 100,
                        trials = trials)

print("The best hyperparameters are : ","\n")
print(best_hyperparams)

rf2 = RandomForestRegressor(max_depth = 15, n_estimators = 68)
np.mean(cross_val_score(rf, X_train, y_train, cv = 5, scoring = 'neg_mean_absolute_error'))
rf2.fit(X_train, y_train)


# Select Final Model  -----------------------------------------------------------------#
import math
print('LS : ', sqrt(mean_squared_error(ls.predict(X_test), y_test)))
print('ridge : ', sqrt(mean_squared_error(ridge.predict(X_test), y_test)))
print('xgb : ', sqrt(mean_squared_error(xgb.predict(X_test), y_test)))
print('xgb1 : ', sqrt(mean_squared_error(xgb1.predict(X_test), y_test)))
print('rf : ', sqrt(mean_squared_error(rf1.predict(X_test), y_test)))
print('rf1 : ', sqrt(mean_squared_error(rf12.predict(X_test), y_test)))


# Feature Importance  -----------------------------------------------------------------#
# 1) Permutation_Importance on full-dataset to idenfity variables: simplest
from sklearn.inspection import permutation_importance
result = permutation_importance(XGBRegressor().fit(df,y), df, y, random_state = 0, scoring='neg_mean_squared_error')
for i in range(1,len(result.importances_mean)):
    print(result.importances_mean[i])

sorted_idx = result.importances_mean.argsort()
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=df.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
plt.show()

# 2) Permutation_Importance cv to idenfity variables: simplest
import eli5
from eli5.sklearn import PermutationImportance
    
svc = XGBRegressor(random_state = 0)
perm = PermutationImportance(svc, cv = 5).fit(np.matrix(df), np.matrix(y))
eli5.show_weights(perm)

# 3) Shapley
import shap  # package used to calculate Shap values

row_to_show = 5 # Shapley Value of specific one observation
data_for_prediction = X_test.iloc[row_to_show]
data_for_prediction_array = data_for_prediction.values.reshape(1, -1)

# Create object that can calculate shap values
explainer = shap.TreeExplainer(rf2)

# Calculate Shap values
shap_values = explainer.shap_values(data_for_prediction)

shap.initjs()
shap.force_plot(explainer.expected_value, shap_values, data_for_prediction)

# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.
shap_values = explainer.shap_values(X_test)

# Make plot. Index of [1] is explained in text below.
shap.summary_plot(shap_values, X_test) # (, max_display = X_test.shape[1])

# make plot.
shap.dependence_plot('Transactions (Txn)', shap_values, X_test, interaction_index="Cons Households (HH)")
