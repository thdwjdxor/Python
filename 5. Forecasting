from IPython.core.display import display, HTML
display(HTML("<style>.container { width:90% !important; }</style>"))

%pylab inline

import dataiku
from dataiku import pandasutils as pdu
import pandas as pd
from datetime import datetime

# Read the dataset as a Pandas dataframe in memory
# Note: here, we only read the first 100K rows. Other sampling options are available
df = dataiku.Dataset("Market_Opportunity_joined").get_dataframe()

# Predict Total Monthly DDA Sales of 524 Branches, goaled in F19

## 1 Univariate: SARIMA

## 1.1 Explore Sales_Count

print('Number of Unique Branches: ', len(df['Row Labels'].unique()))
print('3 BUCs with less than 80% DDA Month Sales data')

df_prep = pd.DataFrame(df.groupby(['YEAR', 'ESSBASE_MONTH', 'CATEGORY'])['SALES_COUNT'].sum()).reset_index()

# # Forecast Specifi Branch
# dfg = df[df['CATEGORY'] == 'Total DDA'][['Row Labels', 'SALES_COUNT']].groupby('Row Labels').count()
# buc = np.array(dfg[dfg == 72].dropna().index)

# dfd = df[df['Row Labels'] == 762]

# df_prep = pd.DataFrame(dfd.groupby(['YEAR', 'ESSBASE_MONTH', 'CATEGORY'])['SALES_COUNT'].mean()).reset_index()

df_dda = df_prep[df_prep['CATEGORY'] == 'Total DDA']
df_dda['Date'] = df_dda.apply(lambda row: datetime(row['YEAR'], row['ESSBASE_MONTH'], 1), axis=1)
df_dda = df_dda[['Date', 'SALES_COUNT']]
df_dda.set_index('Date', inplace=True)

df_dda.head()

plt.figure(figsize=(30,5))
plt.plot(df_dda)

import statsmodels.api as sm  
from statsmodels.tsa.seasonal import seasonal_decompose
result = seasonal_decompose(df_dda, model='multiplicative')
fig = result.plot()
fig.set_size_inches(10, 6);

- Show Upper Trend
- Show Seasonality

## 1.1.1 Stationarity: Sales_Count

### Note: traditional time-series modeling require stationarity of variable

from statsmodels.tsa.stattools import adfuller

#Perform Augmented Dickey–Fuller test:
print('Results of Dickey Fuller Test:')
dftest = adfuller(df_dda['SALES_COUNT'], autolag='AIC')

dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
    
print(dfoutput)

- ADF Test fail to reject the null that Sales_Count is not-stationary. Therefore, necessary transomartion is needed(e.i. log, first-difference, moving-average difference,etc)

## 1.1.2 Stationarity: log(Sales_Count)

df_dda_log = np.log(df_dda)
plt.figure(figsize=(30,5))
plt.plot(df_dda_log)

from statsmodels.tsa.stattools import adfuller

#Perform Augmented Dickey–Fuller test:
print('Results of Dickey Fuller Test:')
dftest = adfuller(df_dda_log['SALES_COUNT'], autolag='AIC')

dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
    
print(dfoutput)

- ADF Test fail to reject the null that log(Sales_Count) is not-stationary.

## 1.1.3 Stationarity: diff(Sales_Count)

df_dda_diff = df_dda.diff().dropna()
plt.figure(figsize=(30,5))
plt.plot(df_dda_diff)

from statsmodels.tsa.stattools import adfuller

#Perform Augmented Dickey–Fuller test:
print('Results of Dickey Fuller Test:')
dftest = adfuller(df_dda_diff['SALES_COUNT'], autolag='AIC')

dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
    
print(dfoutput)

- ADF Test fail to reject the null that diff(Sales_Count) is not-stationary.

## 1.1.4 Stationarity: diff(Sales_Count).Seasonality

df_dda_diff_season = df_dda.diff(12).dropna()
plt.figure(figsize=(30,5))
plt.plot(df_dda_diff_season)

from statsmodels.tsa.stattools import adfuller

#Perform Augmented Dickey–Fuller test:
print('Results of Dickey Fuller Test:')
dftest = adfuller(df_dda_diff_season['SALES_COUNT'], autolag='AIC')

dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
    
print(dfoutput)

- ADF Test fail to reject the null that diff(Sales_Count).season is not-stationary.

## 1.1.5 Stationarity: diff(Sales_Count).Seasonality.diff()

df_dda_diff_season = df_dda.diff(12).dropna().diff().dropna()
plt.figure(figsize=(30,5))
plt.plot(df_dda_diff_season)

from statsmodels.tsa.stattools import adfuller

#Perform Augmented Dickey–Fuller test:
print('Results of Dickey Fuller Test:')
dftest = adfuller(df_dda_diff_season['SALES_COUNT'], autolag='AIC')

dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
    
print(dfoutput)

- ADF Test reject the null that diff(Sales_Count).season is not-stationary.

from statsmodels.tsa.stattools import acf, pacf

#ACF & PACF plots

lag_acf = acf(df_dda_diff_season, nlags=30)
lag_pacf = pacf(df_dda_diff_season, nlags=30, method='ols')

#Plot ACF:
plt.figure(figsize=(20,5))
plt.subplot(121)
plt.plot(lag_acf, marker='o')
plt.axhline(y=0, linestyle='--', color='gray')
plt.axhline(y=-1.96/np.sqrt(len(df_dda_diff_season)), linestyle='--', color='gray')
plt.axhline(y=1.96/np.sqrt(len(df_dda_diff_season)), linestyle='--', color='gray')
plt.title('Autocorrelation Function')            

#Plot PACF
plt.subplot(122)
plt.plot(lag_pacf, marker='o')
plt.axhline(y=0, linestyle='--', color='gray')
plt.axhline(y=-1.96/np.sqrt(len(df_dda_diff_season)), linestyle='--', color='gray')
plt.axhline(y=1.96/np.sqrt(len(df_dda_diff_season)), linestyle='--', color='gray')
plt.title('Partial Autocorrelation Function')
            
plt.tight_layout() 


# grid search sarima hyperparameters
from math import sqrt
from multiprocessing import cpu_count
from joblib import Parallel
from joblib import delayed
from warnings import catch_warnings
from warnings import filterwarnings
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error

# one-step sarima forecast
def sarima_forecast(history, config):
    order, sorder, trend = config
    # define model
    # model = SARIMAX(history, order=order, seasonal_order=sorder, trend=trend, enforce_stationarity=False, enforce_invertibility=False)
    model = SARIMAX(history, order=order, seasonal_order=sorder)
    # fit model
    model_fit = model.fit(disp=False)
    # make one step forecast
    yhat = model_fit.predict(len(history), len(history))
    return yhat[0]

# root mean squared error or rmse
def measure_rmse(actual, predicted):
    return sqrt(mean_squared_error(actual, predicted))

# split a univariate dataset into train/test sets
def train_test_split(data, n_test):
    return data[:-n_test], data[-n_test:]

# walk-forward validation for univariate data
def walk_forward_validation(data, n_test, cfg):
    predictions = list()
    # split dataset
    train, test = train_test_split(data, n_test)
    # seed history with training dataset
    history = [x for x in train]
    # step over each time-step in the test set
    for i in range(len(test)):
        # fit model and make forecast for history
        yhat = sarima_forecast(history, cfg)
        # store forecast in list of predictions
        predictions.append(yhat)
        # add actual observation to history for the next loop
        history.append(test[i])
    # estimate prediction error
    error = measure_rmse(test, predictions)
    return error

# score a model, return None on failure
def score_model(data, n_test, cfg, debug=False):
    result = None
    # convert config to a key
    key = str(cfg)
    # show all warnings and fail on exception if debugging
    if debug:
        result = walk_forward_validation(data, n_test, cfg)
    else:
        # one failure during model validation suggests an unstable config
        try:
            # never show warnings when grid searching, too noisy
            with catch_warnings():
                filterwarnings("ignore")
                result = walk_forward_validation(data, n_test, cfg)
        except:
            error = None
    # check for an interesting result
    if result is not None:
        print(' > Model[%s] %.3f' % (key, result))
    return (key, result)

# grid search configs
def grid_search(data, cfg_list, n_test, parallel=True):
    scores = None
    if parallel:
        # execute configs in parallel
        executor = Parallel(n_jobs=cpu_count(), backend='multiprocessing')
        tasks = (delayed(score_model)(data, n_test, cfg) for cfg in cfg_list)
        scores = executor(tasks)
    else:
        scores = [score_model(data, n_test, cfg) for cfg in cfg_list]
    # remove empty results
    scores = [r for r in scores if r[1] != None]
    # sort configs by error, asc
    scores.sort(key=lambda tup: tup[1])
    return scores

# create a set of sarima configs to try
def sarima_configs(seasonal=[12]):
    models = list()
    # define config lists
    p_params = [0, 1, 2]
    d_params = [1]
    q_params = [0, 1, 2]
    t_params = ['c','t','ct']
    P_params = [0, 1, 2]
    D_params = [1]
    Q_params = [0, 1, 2]
    m_params = seasonal
    # create config instances
    for p in p_params:
        for d in d_params:
            for q in q_params:
                for t in t_params:
                    for P in P_params:
                        for D in D_params:
                            for Q in Q_params:
                                for m in m_params:
                                    cfg = [(p,d,q), (P,D,Q,m), t]
                                    models.append(cfg)
    return models

if __name__ == '__main__':
    # define dataset
    data = df_dda.iloc[:,0]
    print(data)
    # data split
    n_test = 15
    # model configs
    cfg_list = sarima_configs()
    # grid search
    scores = grid_search(data, cfg_list, n_test)
    print('done')
    # list top 3 configs
    for cfg, error in scores[:3]:
        print(cfg, error)

# grid search sarima hyperparameters
from math import sqrt
from multiprocessing import cpu_count
from joblib import Parallel
from joblib import delayed
from warnings import catch_warnings
from warnings import filterwarnings
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error

# load dataset
series = df_dda.iloc[:,0]
# split into train and test sets
X = series.values
train, test = X[0:-15], X[-15:]
history = list(train)
predictions = list()

# walk-forward validation
for t in range(len(test)):
    # fit model
    model = SARIMAX(history, order=[1,1,2], seasonal_order=[1,1,0,12], trend='c')
    model_fit = model.fit()
    # one step forecast
    yhat = model_fit.predict(len(history), len(history))
    # store forecast and ob
    predictions.append(yhat)
    history.append(test[t])
    
# evaluate forecasts
rmse = sqrt(mean_squared_error(test, predictions))
print('Test RMSE: %.3f' % rmse)
# plot forecasts against actual outcomes
pyplot.plot(test)
pyplot.plot(predictions, color='red')
pyplot.show()

print(model_fit.summary())

# 2 Univeriate: Simple ES

def walk_forward_validation(data, n_test):
    predictions = list()
    # split dataset
    train, test = train_test_split(data, n_test)
    # seed history with training dataset
    history = [x for x in train]
    # step over each time-step in the test set
    for i in range(len(test)):
        # fit model and make forecast for history
        model = SimpleExpSmoothing(history)
        model_fit = model.fit()
        yhat = model_fit.predict(len(data), len(data))
        # store forecast in list of predictions
        predictions.append(yhat)
        # add actual observation to history for the next loop
        history.append(test[i])
    # estimate prediction error
    error = measure_rmse(test, predictions)
    return error

# SES example
from statsmodels.tsa.holtwinters import SimpleExpSmoothing
from random import random
# contrived dataset
data = df_dda.iloc[:,0]

walk_forward_validation(data, 15)

# load dataset
series = df_dda.iloc[:,0]
# split into train and test sets
X = series.values
train, test = X[0:-15], X[-15:]
history = list(train)
predictions = list()

# walk-forward validation
for t in range(len(test)):
    # fit model
    model = SimpleExpSmoothing(history)
    model_fit = model.fit()
    yhat = model_fit.predict(len(history), len(history))
    # store forecast and ob
    predictions.append(yhat)
    history.append(test[t])
    
# evaluate forecasts
rmse = sqrt(mean_squared_error(test, predictions))
print('Test RMSE: %.3f' % rmse)
# plot forecasts against actual outcomes
pyplot.plot(test)
pyplot.plot(predictions, color='red')
pyplot.show()

# 3 Univeriate: Holt Winter’s Exponential Smoothing (HWES)

# grid search holt winter's exponential smoothing
from math import sqrt
from multiprocessing import cpu_count
from joblib import Parallel
from joblib import delayed
from warnings import catch_warnings
from warnings import filterwarnings
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.metrics import mean_squared_error
from numpy import array
 
# one-step Holt Winter’s Exponential Smoothing forecast
def exp_smoothing_forecast(history, config):
	t,d,s,p,b,r = config
	# define model
	history = array(history)
	model = ExponentialSmoothing(history, trend=t, damped=d, seasonal=s, seasonal_periods=p)
	# fit model
	model_fit = model.fit(optimized=True, use_boxcox=b, remove_bias=r)
	# make one step forecast
	yhat = model_fit.predict(len(history), len(history))
	return yhat[0]
 
# root mean squared error or rmse
def measure_rmse(actual, predicted):
	return sqrt(mean_squared_error(actual, predicted))
 
# split a univariate dataset into train/test sets
def train_test_split(data, n_test):
	return data[:-n_test], data[-n_test:]
 
# walk-forward validation for univariate data
def walk_forward_validation(data, n_test, cfg):
	predictions = list()
	# split dataset
	train, test = train_test_split(data, n_test)
	# seed history with training dataset
	history = [x for x in train]
	# step over each time-step in the test set
	for i in range(len(test)):
		# fit model and make forecast for history
		yhat = exp_smoothing_forecast(history, cfg)
		# store forecast in list of predictions
		predictions.append(yhat)
		# add actual observation to history for the next loop
		history.append(test[i])
	# estimate prediction error
	error = measure_rmse(test, predictions)
	return error
 
# score a model, return None on failure
def score_model(data, n_test, cfg, debug=False):
	result = None
	# convert config to a key
	key = str(cfg)
	# show all warnings and fail on exception if debugging
	if debug:
		result = walk_forward_validation(data, n_test, cfg)
	else:
		# one failure during model validation suggests an unstable config
		try:
			# never show warnings when grid searching, too noisy
			with catch_warnings():
				filterwarnings("ignore")
				result = walk_forward_validation(data, n_test, cfg)
		except:
			error = None
	# check for an interesting result
	if result is not None:
		print(' > Model[%s] %.3f' % (key, result))
	return (key, result)
 
# grid search configs
def grid_search(data, cfg_list, n_test, parallel=True):
	scores = None
	if parallel:
		# execute configs in parallel
		executor = Parallel(n_jobs=cpu_count(), backend='multiprocessing')
		tasks = (delayed(score_model)(data, n_test, cfg) for cfg in cfg_list)
		scores = executor(tasks)
	else:
		scores = [score_model(data, n_test, cfg) for cfg in cfg_list]
	# remove empty results
	scores = [r for r in scores if r[1] != None]
	# sort configs by error, asc
	scores.sort(key=lambda tup: tup[1])
	return scores
 
# create a set of exponential smoothing configs to try
def exp_smoothing_configs(seasonal=[None]):
	models = list()
	# define config lists
	t_params = ['add', 'mul', None]
	d_params = [True, False]
	s_params = ['add', 'mul', None]
	p_params = seasonal
	b_params = [True, False]
	r_params = [True, False]
	# create config instances
	for t in t_params:
		for d in d_params:
			for s in s_params:
				for p in p_params:
					for b in b_params:
						for r in r_params:
							cfg = [t,d,s,p,b,r]
							models.append(cfg)
	return models
 
if __name__ == '__main__':
	# define dataset
	data = df_dda.iloc[:,0]
	# data split
	n_test = 15
	# model configs
	cfg_list = exp_smoothing_configs()
	# grid search
	scores = grid_search(data, cfg_list, n_test)
	print('done')
	# list top 3 configs
	for cfg, error in scores[:3]:
		print(cfg, error)

# load dataset
series = df_dda.iloc[:,0]
# split into train and test sets
X = series.values
train, test = X[0:-15], X[-15:]
history = list(train)
predictions = list()

# walk-forward validation
for t in range(len(test)):
    # fit model
    model = ExponentialSmoothing(data,  trend='mul', damped=True, seasonal=None, seasonal_periods=None)
    model_fit = model.fit(optimized=True, use_boxcox=False, remove_bias=False)
    # make prediction
    yhat = model_fit.predict(len(history), len(history))
    # store forecast and ob
    predictions.append(yhat)
    history.append(test[t])
    
# evaluate forecasts
rmse = sqrt(mean_squared_error(test, predictions))
print('Test RMSE: %.3f' % rmse)
# plot forecasts against actual outcomes
pyplot.plot(test)
pyplot.plot(predictions, color='red')
pyplot.show()



df_dda_diff = df_dda.diff().dropna()
plt.plot(df_dda_diff)

from numpy import array

# split a univariate sequence into samples
def split_sequence(sequence, n_steps):
	X, y = list(), list()
	for i in range(len(sequence)):
		# find the end of this pattern
		end_ix = i + n_steps
		# check if we are beyond the sequence
		if end_ix > len(sequence)-1:
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)

# define input sequence
raw_seq = df_dda.iloc[:,0]
# choose a number of time steps
n_steps = 3
# split into samples
X, y = split_sequence(raw_seq, n_steps)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.23, shuffle = False)

from sklearn.linear_model import LinearRegression
ls = LinearRegression()
ls.fit(X_train, y_train)
print(np.sqrt(mean_squared_error(ls.predict(X_test),y_test)))

from sklearn.linear_model import Ridge
ridge = Ridge(alpha = 0.9)
ridge.fit(X_train, y_train)
print(np.sqrt(mean_squared_error(ridge.predict(X_test),y_test)))

from xgboost import XGBRegressor
xgb = XGBRegressor()
xgb.fit(X_train, y_train)
print(np.sqrt(mean_squared_error(xgb.predict(X_test),y_test)))

from sklearn.ensemble import RandomForestRegressor
rf1 = RandomForestRegressor()
rf1.fit(X_train, y_train)
print(np.sqrt(mean_squared_error(rf1.predict(X_test),y_test)))

rmse = sqrt(mean_squared_error(xgb.predict(X_test),y_test))
print('Test RMSE: %.3f' % rmse)

pyplot.plot(y_test)
pyplot.plot(xgb.predict(X_test), color='red')
pyplot.show()
